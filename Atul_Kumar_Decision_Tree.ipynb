{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "wkgiX6YNn89F",
        "outputId": "171de1eb-54ac-42fa-b5b9-ad5a7d8b0979"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n## Question 1: What is a Decision Tree, and how does it work in the context of classification?\\n\\nA decision tree is a supervised learning model resembling a tree (or flowchart) structure, used for classification (and regression) tasks. In the classification context, it partitions the feature space into regions associated with distinct class labels.\\n\\nStructure of a Decision Tree\\n\\nRoot node: the topmost node that contains the entire training dataset.\\n\\nInternal (decision) nodes: nodes where a test is applied on one of the features (e.g. “Is feature X > threshold?”).\\n\\nBranches / edges: outcomes of the test, which lead to child nodes.\\n\\nLeaf (terminal) nodes: nodes that assign a class label (final decision).\\n\\nThe path from the root to a leaf corresponds to a rule: a conjunction of feature‑tests that leads to a classification decision. \\n\\nHow It Works for Classification\\n\\nRecursive splitting (“divide and conquer”)\\nStarting at the root, the algorithm selects a feature and a split (e.g. threshold) that best partitions the data into purer subsets (i.e. subsets where one class predominates). This process is repeated on each subset, recursively creating child nodes. \\n\\nImpurity / splitting criterion\\nTo decide which split is “best,” decision tree algorithms use impurity or information measures such as:\\n\\nEntropy / Information Gain: how much uncertainty is reduced by the split \\n\\n\\nGini impurity: probability of misclassifying a randomly chosen instance if labeled according to class proportions in the node \\n\\n\\nThe algorithm evaluates candidate splits and picks the one that leads to the highest gain (or the greatest impurity reduction). \\n\\n\\nStopping / leaf creation\\nThe splitting continues until a stopping criterion is met, for example:\\n\\nall instances in that node belong to the same class,\\n\\nno further features remain for splitting,\\n\\nor a maximum tree depth / minimum samples per node constraint is reached (to prevent overfitting).\\nAt that point, the node becomes a leaf and is assigned a class (often the majority class among the training instances in that node). \\n\\n\\nPrediction on new instances\\nGiven a new data sample, we start at the root and evaluate the feature test there. Depending on the outcome, we follow the appropriate branch. We continue until a leaf node is reached, and we output the class label stored in that leaf as the prediction. \\n\\nStrengths and Limitations (brief)\\n\\nStrengths\\n\\nHighly interpretable — one can trace the path of decisions easily.\\n\\nHandles both categorical and numerical features.\\n\\nLittle preprocessing (no requirement for scaling).\\n\\nCan model non‑linear relationships.\\n\\nLimitations\\n\\nProne to overfitting when grown deep.\\n\\nSensitive to small changes in data (unstable).\\n\\nBiased towards features with many levels/categories.\\n\\nWhen decision boundaries are complex or not axis-aligned, a tree may need many splits, reducing generalization.\\n\\nTo mitigate overfitting, one often uses pruning (cutting back unnecessary branches) or constraints like maximum depth, minimum samples per split, etc.\\n\\n## Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\\n\\nWhat are they (in simple terms)\\n\\nGini impurity is a measure of how “mixed” the classes are in a node. It gives the probability that a randomly chosen example from that node would be mislabeled if we randomly labeled it according to the class proportions in that node. The more mixed the node (i.e. classes are evenly distributed), the higher the impurity. If all examples in the node belong to one class, Gini impurity is zero (i.e. the node is “pure”). \\n\\nEntropy (from information theory) measures the amount of uncertainty or disorder in the class distribution of a node. A node with a balanced mix of classes has high entropy (high uncertainty), while a node dominated by a single class has low entropy (low surprise). When all examples in a node are of the same class, entropy is zero — no uncertainty remains. \\n\\nHow they impact splits in a decision tree\\n\\nEvaluating a candidate split\\nAt each node, the tree algorithm considers possible ways to divide the data (by features and thresholds). For each candidate split, it looks at how “impure” the two resulting child nodes would be (using Gini or entropy), and also accounts for how many examples go into each child (i.e. weighted by size).\\n\\nChoosing the best split\\nThe algorithm picks the split that yields the largest reduction in impurity (i.e. the greatest “purity gain”). In other words, it chooses the division that makes the children as “pure” as possible (lowest impurity) while also keeping the split balanced or meaningful. For entropy, this is often expressed as information gain (how much uncertainty is removed by the split). \\n\\nDifferences in behavior / biases\\n\\nComputational cost: Entropy involves logarithmic operations, which are relatively more expensive; Gini impurity uses simpler operations, making it slightly faster in practice. \\n\\nSplit preference: Gini tends to prefer splits that isolate the most frequent class more aggressively, aiming to reduce impurity quickly. Entropy is somewhat more sensitive to class distribution and may produce slightly more balanced splits in some cases. But in many real-world datasets, both criteria often lead to highly similar trees. \\n\\nBecause differences are often small, many decision tree implementations (such as CART) use Gini by default for efficiency.\\n\\n\\n## Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision \\nTrees? Give one practical advantage of using each.\\n\\nPre‑Pruning (Early Stopping)\\n\\nWhat it is:\\nPre‑pruning means you stop the growth of the decision tree during its construction, rather than growing it fully. You impose constraints or criteria so that splits are only performed if they satisfy certain thresholds. Common constraints include: maximum tree depth, minimum number of samples required to split a node, minimum impurity improvement (or gain) needed for a split, minimum samples per leaf, etc. \\n\\nPractical advantage:\\nBecause you limit the size of the tree from the start, training is faster and less resource‑intensive. You avoid growing lots of branches that may be largely irrelevant or noisy. This can be especially valuable when dealing with large datasets or limited computational resources. Also simpler trees are easier to interpret early. \\n\\nPost‑Pruning\\n\\nWhat it is:\\nPost‑pruning (sometimes called “prune after full growth”) means you let the tree grow completely (or at least without strong early constraints), potentially overfitting the training data, and then afterwards remove (\"prune\") branches or subtrees that don’t help generalization. Pruning is based on evaluating performance (often on validation data) or using cost‑complexity criteria that trade off tree complexity vs. error. Methods include reduced error pruning, cost‑complexity pruning, etc. \\n\\nPractical advantage:\\nPost‑pruning tends to yield trees that generalize better because you can first see the full complexity and only then trim away those parts that are truly unnecessary. This helps in capturing subtle patterns in the data that pre‑pruning might block prematurely. It often gives a better balance between bias and variance. \\n\\nKey Differences & Trade‑Offs\\n\\nWhen the decision to prune or stop comes: pre‑pruning acts during growth, post‑pruning acts after. \\n\\nRisk: pre‑pruning risks underfitting (missing useful structure because you cut off splits too early). Post‑pruning risks greater computational work, since you build a large tree first and then evaluate many possible prunings.\\n\\n\\n## Question 4: What is Information Gain in Decision Trees, and why is it important for \\nchoosing the best split? \\n\\nInformation Gain in decision trees is a measure of how much “useful information” a feature gives us about the target class. In other words, it quantifies the reduction in uncertainty (or impurity) about the class labels when we split a node using that feature. It is the difference between the impurity before the split and the weighted impurity after the split. \\n\\nWhy Information Gain matters for choosing splits\\n\\nGuides the best split choice\\nAt each internal node, the decision tree algorithm considers multiple candidate features (and possible split points). Information Gain provides a quantitative criterion: the split that yields the highest information gain is chosen, because it leads to the greatest reduction in class uncertainty (i.e. the most “pure” children). \\n\\nEncourages purer child nodes\\nA high information gain indicates that after splitting, the resulting child nodes have more homogeneous class distributions compared to the parent. In effect, the split separates classes well. Thus, features that produce splits with strong separation will tend to have higher gain. \\n\\nHelps in building efficient, interpretable trees\\nBecause information gain favors splits that maximize purity early, the tree tends to place more informative features higher up (closer to the root). This leads to more compact and interpretable trees (since early splits already separate many classes). \\n\\nRecognizing limits and biases\\nOne downside is that information gain is biased toward features with many distinct values (e.g. unique identifiers), because they might artificially produce pure splits. To counteract this, variants like information gain ratio are used (which penalize features with many splits).\\n\\n\\n## Question 5: What are some common real-world applications of Decision Trees, and \\nwhat are their main advantages and limitations? \\n\\nReal‑World Applications\\n\\nHealthcare / Medical Diagnosis\\nDecision trees are used to help diagnose diseases or predict patient outcomes using features such as symptoms, medical history, lab test results. They’re valued especially where interpretability is critical (doctors need to see “why” a prediction is made). KNOWRA\\n\\nFinance & Credit Scoring\\nBanks and financial institutions use decision trees to assess credit risk of loan applicants, to detect fraud, or to decide whether a transaction is suspicious. \\n\\nMarketing & Customer Segmentation\\nIn marketing, decision trees help categorize customers based on behavior, demographics, likelihood to respond, churn risk, etc. They support targeted campaigns, retention strategies. \\n\\nPredictive Maintenance in Manufacturing\\nThey monitor machinery/workflow data to predict failures and schedule maintenance before breakdowns. This reduces downtime and can save cost. \\n\\nFraud Detection / Anomaly Detection\\nDetecting unusual or fraudulent behavior (in transactions, insurance claims, etc.) is a common use case because trees can learn rules that separate normal vs abnormal patterns. \\n\\nMain Advantages\\n\\nInterpretability / Transparency: The “if‑then” rules are easy to follow; stakeholders can understand how a decision is made. Useful in domains where explanations are necessary (healthcare, finance). \\n\\nHandles different data kinds: Trees can work with both categorical and numerical features without heavy preprocessing like scaling or normalization. Also useful when there are missing values. \\n\\nNon‑linear relationships: They can capture nonlinear decision boundaries and variable interactions without having to manually specify interaction terms. \\n\\nMain Limitations\\n\\nOverfitting: If allowed to grow too deep, a tree may fit noise in training data, reducing its performance on new/unseen data. Pruning, limiting depth, etc., are mitigation methods. \\n\\nInstability / High Variance: Small changes in the training data (e.g. one or few examples added/removed) can lead to very different tree structures and predictions. This reduces reliability in some settings. \\n\\nBias with Imbalanced Data / Features with Many Levels: Trees may favor features with many distinct values, and may underperform when one class is heavily underrepresented (since splits tend to favor majority class). \\n\\nLimited Smoothness / Granularity: Predictions are piecewise constant (for classification) or piecewise constant/regional (for regression). In some applications, smoother or more continuous models are needed. Also, very complex trees are hard to interpret.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "## Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "A decision tree is a supervised learning model resembling a tree (or flowchart) structure, used for classification (and regression) tasks. In the classification context, it partitions the feature space into regions associated with distinct class labels.\n",
        "\n",
        "Structure of a Decision Tree\n",
        "\n",
        "Root node: the topmost node that contains the entire training dataset.\n",
        "\n",
        "Internal (decision) nodes: nodes where a test is applied on one of the features (e.g. “Is feature X > threshold?”).\n",
        "\n",
        "Branches / edges: outcomes of the test, which lead to child nodes.\n",
        "\n",
        "Leaf (terminal) nodes: nodes that assign a class label (final decision).\n",
        "\n",
        "The path from the root to a leaf corresponds to a rule: a conjunction of feature‑tests that leads to a classification decision.\n",
        "\n",
        "How It Works for Classification\n",
        "\n",
        "Recursive splitting (“divide and conquer”)\n",
        "Starting at the root, the algorithm selects a feature and a split (e.g. threshold) that best partitions the data into purer subsets (i.e. subsets where one class predominates). This process is repeated on each subset, recursively creating child nodes.\n",
        "\n",
        "Impurity / splitting criterion\n",
        "To decide which split is “best,” decision tree algorithms use impurity or information measures such as:\n",
        "\n",
        "Entropy / Information Gain: how much uncertainty is reduced by the split\n",
        "\n",
        "\n",
        "Gini impurity: probability of misclassifying a randomly chosen instance if labeled according to class proportions in the node\n",
        "\n",
        "\n",
        "The algorithm evaluates candidate splits and picks the one that leads to the highest gain (or the greatest impurity reduction).\n",
        "\n",
        "\n",
        "Stopping / leaf creation\n",
        "The splitting continues until a stopping criterion is met, for example:\n",
        "\n",
        "all instances in that node belong to the same class,\n",
        "\n",
        "no further features remain for splitting,\n",
        "\n",
        "or a maximum tree depth / minimum samples per node constraint is reached (to prevent overfitting).\n",
        "At that point, the node becomes a leaf and is assigned a class (often the majority class among the training instances in that node).\n",
        "\n",
        "\n",
        "Prediction on new instances\n",
        "Given a new data sample, we start at the root and evaluate the feature test there. Depending on the outcome, we follow the appropriate branch. We continue until a leaf node is reached, and we output the class label stored in that leaf as the prediction.\n",
        "\n",
        "Strengths and Limitations (brief)\n",
        "\n",
        "Strengths\n",
        "\n",
        "Highly interpretable — one can trace the path of decisions easily.\n",
        "\n",
        "Handles both categorical and numerical features.\n",
        "\n",
        "Little preprocessing (no requirement for scaling).\n",
        "\n",
        "Can model non‑linear relationships.\n",
        "\n",
        "Limitations\n",
        "\n",
        "Prone to overfitting when grown deep.\n",
        "\n",
        "Sensitive to small changes in data (unstable).\n",
        "\n",
        "Biased towards features with many levels/categories.\n",
        "\n",
        "When decision boundaries are complex or not axis-aligned, a tree may need many splits, reducing generalization.\n",
        "\n",
        "To mitigate overfitting, one often uses pruning (cutting back unnecessary branches) or constraints like maximum depth, minimum samples per split, etc.\n",
        "\n",
        "## Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "What are they (in simple terms)\n",
        "\n",
        "Gini impurity is a measure of how “mixed” the classes are in a node. It gives the probability that a randomly chosen example from that node would be mislabeled if we randomly labeled it according to the class proportions in that node. The more mixed the node (i.e. classes are evenly distributed), the higher the impurity. If all examples in the node belong to one class, Gini impurity is zero (i.e. the node is “pure”).\n",
        "\n",
        "Entropy (from information theory) measures the amount of uncertainty or disorder in the class distribution of a node. A node with a balanced mix of classes has high entropy (high uncertainty), while a node dominated by a single class has low entropy (low surprise). When all examples in a node are of the same class, entropy is zero — no uncertainty remains.\n",
        "\n",
        "How they impact splits in a decision tree\n",
        "\n",
        "Evaluating a candidate split\n",
        "At each node, the tree algorithm considers possible ways to divide the data (by features and thresholds). For each candidate split, it looks at how “impure” the two resulting child nodes would be (using Gini or entropy), and also accounts for how many examples go into each child (i.e. weighted by size).\n",
        "\n",
        "Choosing the best split\n",
        "The algorithm picks the split that yields the largest reduction in impurity (i.e. the greatest “purity gain”). In other words, it chooses the division that makes the children as “pure” as possible (lowest impurity) while also keeping the split balanced or meaningful. For entropy, this is often expressed as information gain (how much uncertainty is removed by the split).\n",
        "\n",
        "Differences in behavior / biases\n",
        "\n",
        "Computational cost: Entropy involves logarithmic operations, which are relatively more expensive; Gini impurity uses simpler operations, making it slightly faster in practice.\n",
        "\n",
        "Split preference: Gini tends to prefer splits that isolate the most frequent class more aggressively, aiming to reduce impurity quickly. Entropy is somewhat more sensitive to class distribution and may produce slightly more balanced splits in some cases. But in many real-world datasets, both criteria often lead to highly similar trees.\n",
        "\n",
        "Because differences are often small, many decision tree implementations (such as CART) use Gini by default for efficiency.\n",
        "\n",
        "\n",
        "## Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Pre‑Pruning (Early Stopping)\n",
        "\n",
        "What it is:\n",
        "Pre‑pruning means you stop the growth of the decision tree during its construction, rather than growing it fully. You impose constraints or criteria so that splits are only performed if they satisfy certain thresholds. Common constraints include: maximum tree depth, minimum number of samples required to split a node, minimum impurity improvement (or gain) needed for a split, minimum samples per leaf, etc.\n",
        "\n",
        "Practical advantage:\n",
        "Because you limit the size of the tree from the start, training is faster and less resource‑intensive. You avoid growing lots of branches that may be largely irrelevant or noisy. This can be especially valuable when dealing with large datasets or limited computational resources. Also simpler trees are easier to interpret early.\n",
        "\n",
        "Post‑Pruning\n",
        "\n",
        "What it is:\n",
        "Post‑pruning (sometimes called “prune after full growth”) means you let the tree grow completely (or at least without strong early constraints), potentially overfitting the training data, and then afterwards remove (\"prune\") branches or subtrees that don’t help generalization. Pruning is based on evaluating performance (often on validation data) or using cost‑complexity criteria that trade off tree complexity vs. error. Methods include reduced error pruning, cost‑complexity pruning, etc.\n",
        "\n",
        "Practical advantage:\n",
        "Post‑pruning tends to yield trees that generalize better because you can first see the full complexity and only then trim away those parts that are truly unnecessary. This helps in capturing subtle patterns in the data that pre‑pruning might block prematurely. It often gives a better balance between bias and variance.\n",
        "\n",
        "Key Differences & Trade‑Offs\n",
        "\n",
        "When the decision to prune or stop comes: pre‑pruning acts during growth, post‑pruning acts after.\n",
        "\n",
        "Risk: pre‑pruning risks underfitting (missing useful structure because you cut off splits too early). Post‑pruning risks greater computational work, since you build a large tree first and then evaluate many possible prunings.\n",
        "\n",
        "\n",
        "## Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Information Gain in decision trees is a measure of how much “useful information” a feature gives us about the target class. In other words, it quantifies the reduction in uncertainty (or impurity) about the class labels when we split a node using that feature. It is the difference between the impurity before the split and the weighted impurity after the split.\n",
        "\n",
        "Why Information Gain matters for choosing splits\n",
        "\n",
        "Guides the best split choice\n",
        "At each internal node, the decision tree algorithm considers multiple candidate features (and possible split points). Information Gain provides a quantitative criterion: the split that yields the highest information gain is chosen, because it leads to the greatest reduction in class uncertainty (i.e. the most “pure” children).\n",
        "\n",
        "Encourages purer child nodes\n",
        "A high information gain indicates that after splitting, the resulting child nodes have more homogeneous class distributions compared to the parent. In effect, the split separates classes well. Thus, features that produce splits with strong separation will tend to have higher gain.\n",
        "\n",
        "Helps in building efficient, interpretable trees\n",
        "Because information gain favors splits that maximize purity early, the tree tends to place more informative features higher up (closer to the root). This leads to more compact and interpretable trees (since early splits already separate many classes).\n",
        "\n",
        "Recognizing limits and biases\n",
        "One downside is that information gain is biased toward features with many distinct values (e.g. unique identifiers), because they might artificially produce pure splits. To counteract this, variants like information gain ratio are used (which penalize features with many splits).\n",
        "\n",
        "\n",
        "## Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Real‑World Applications\n",
        "\n",
        "Healthcare / Medical Diagnosis\n",
        "Decision trees are used to help diagnose diseases or predict patient outcomes using features such as symptoms, medical history, lab test results. They’re valued especially where interpretability is critical (doctors need to see “why” a prediction is made). KNOWRA\n",
        "\n",
        "Finance & Credit Scoring\n",
        "Banks and financial institutions use decision trees to assess credit risk of loan applicants, to detect fraud, or to decide whether a transaction is suspicious.\n",
        "\n",
        "Marketing & Customer Segmentation\n",
        "In marketing, decision trees help categorize customers based on behavior, demographics, likelihood to respond, churn risk, etc. They support targeted campaigns, retention strategies.\n",
        "\n",
        "Predictive Maintenance in Manufacturing\n",
        "They monitor machinery/workflow data to predict failures and schedule maintenance before breakdowns. This reduces downtime and can save cost.\n",
        "\n",
        "Fraud Detection / Anomaly Detection\n",
        "Detecting unusual or fraudulent behavior (in transactions, insurance claims, etc.) is a common use case because trees can learn rules that separate normal vs abnormal patterns.\n",
        "\n",
        "Main Advantages\n",
        "\n",
        "Interpretability / Transparency: The “if‑then” rules are easy to follow; stakeholders can understand how a decision is made. Useful in domains where explanations are necessary (healthcare, finance).\n",
        "\n",
        "Handles different data kinds: Trees can work with both categorical and numerical features without heavy preprocessing like scaling or normalization. Also useful when there are missing values.\n",
        "\n",
        "Non‑linear relationships: They can capture nonlinear decision boundaries and variable interactions without having to manually specify interaction terms.\n",
        "\n",
        "Main Limitations\n",
        "\n",
        "Overfitting: If allowed to grow too deep, a tree may fit noise in training data, reducing its performance on new/unseen data. Pruning, limiting depth, etc., are mitigation methods.\n",
        "\n",
        "Instability / High Variance: Small changes in the training data (e.g. one or few examples added/removed) can lead to very different tree structures and predictions. This reduces reliability in some settings.\n",
        "\n",
        "Bias with Imbalanced Data / Features with Many Levels: Trees may favor features with many distinct values, and may underperform when one class is heavily underrepresented (since splits tend to favor majority class).\n",
        "\n",
        "Limited Smoothness / Granularity: Predictions are piecewise constant (for classification) or piecewise constant/regional (for regression). In some applications, smoother or more continuous models are needed. Also, very complex trees are hard to interpret.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Info:\n",
        "'''● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV). '''\n",
        "#Question 6:   Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier using the Gini criterion\n",
        "#● Print the model’s accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data       # feature matrix (shape: n_samples × n_features)\n",
        "y = iris.target     # class labels (0, 1, 2 for three Iris species)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Instantiate Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test accuracy: {acc:.3f}\")\n",
        "\n",
        "# Feature importances\n",
        "importances = clf.feature_importances_\n",
        "for feat_name, imp in zip(iris.feature_names, importances):\n",
        "    print(f\"{feat_name}: {imp:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y2I-3MvqXjA",
        "outputId": "11e88f5a-a276-4e78-9bd5-999caa2c17ed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 1.000\n",
            "sepal length (cm): 0.000\n",
            "sepal width (cm): 0.019\n",
            "petal length (cm): 0.893\n",
            "petal width (cm): 0.088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7:  Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "#a fully-grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 1. Decision Tree with max_depth = 3\n",
        "clf_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "print(f\"Accuracy with max_depth=3: {acc_limited:.3f}\")\n",
        "\n",
        "# 2. Fully grown Decision Tree (default settings)\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of fully grown tree: {acc_full:.3f}\")\n",
        "\n",
        "# (Optional) Also print feature importances for both\n",
        "print(\"\\nFeature importances (max_depth=3):\")\n",
        "for name, imp in zip(iris.feature_names, clf_limited.feature_importances_):\n",
        "    print(f\"  {name}: {imp:.3f}\")\n",
        "\n",
        "print(\"\\nFeature importances (full tree):\")\n",
        "for name, imp in zip(iris.feature_names, clf_full.feature_importances_):\n",
        "    print(f\"  {name}: {imp:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FdPkX7vqmLq",
        "outputId": "284f2e39-718c-4529-fe1c-74e50de65bed"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.000\n",
            "Accuracy of fully grown tree: 1.000\n",
            "\n",
            "Feature importances (max_depth=3):\n",
            "  sepal length (cm): 0.000\n",
            "  sepal width (cm): 0.000\n",
            "  petal length (cm): 0.925\n",
            "  petal width (cm): 0.075\n",
            "\n",
            "Feature importances (full tree):\n",
            "  sepal length (cm): 0.000\n",
            "  sepal width (cm): 0.019\n",
            "  petal length (cm): 0.893\n",
            "  petal width (cm): 0.088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Load the California Housing dataset from sklearn\n",
        "#● Train a Decision Tree Regressor\n",
        "#● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load data\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# Split into train / test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Instantiate and train the regressor\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Compute Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on test data: {mse:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "importances = reg.feature_importances_\n",
        "print(\"Feature importances:\")\n",
        "for name, imp in zip(feature_names, importances):\n",
        "    print(f\"  {name}: {imp:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BESCeGNeq-yL",
        "outputId": "21352a0b-9786-46eb-e58d-963ce7a253d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on test data: 0.5280\n",
            "Feature importances:\n",
            "  MedInc: 0.5235\n",
            "  HouseAge: 0.0521\n",
            "  AveRooms: 0.0494\n",
            "  AveBedrms: 0.0250\n",
            "  Population: 0.0322\n",
            "  AveOccup: 0.1390\n",
            "  Latitude: 0.0900\n",
            "  Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "#● Load the Iris Dataset\n",
        "#● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "#GridSearchCV\n",
        "#● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "# Initialize the DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10, None],\n",
        "    'min_samples_split': [2, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy: {accuracy:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9s8zKhXrQAB",
        "outputId": "33e203a0-5fcb-4365-9fea-adf5293a4f75"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "Test set accuracy: 0.980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "## Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "\n",
        "\n",
        "To develop a predictive model for disease diagnosis using a healthcare dataset with mixed data types and missing values, follow these steps:\n",
        "\n",
        "1. Handle Missing Values\n",
        "\n",
        "Utilize the MissForest algorithm, a nonparametric imputation method, to handle missing data in mixed-type datasets. MissForest iteratively imputes missing values using a random forest, effectively capturing complex relationships between variables. This approach has been shown to outperform other imputation methods, especially when dealing with nonlinear interactions\n",
        "\n",
        "2. Encode Categorical Features\n",
        "\n",
        "Apply one-hot encoding to transform categorical variables into binary columns. This method ensures that the model interprets categorical data appropriately, preventing any ordinal relationships from being inferred where none exist. For example, convert a 'Gender' column with values 'Male' and 'Female' into two separate columns: 'Gender_Male' and 'Gender_Female'.\n",
        "\n",
        "3. Train a Decision Tree Model\n",
        "\n",
        "Use a DecisionTreeClassifier from scikit-learn to train the model. Decision trees are suitable for healthcare data due to their interpretability and ability to handle both numerical and categorical features. Ensure to set parameters like max_depth and min_samples_split to prevent overfitting and enhance generalization.\n",
        "\n",
        "4. Tune Hyperparameters\n",
        "\n",
        "Implement GridSearchCV to perform an exhaustive search over specified parameter values, such as max_depth, min_samples_split, and min_samples_leaf. This method evaluates all combinations of parameters using cross-validation, ensuring the selection of the optimal model configuration. Alternatively, RandomizedSearchCV can be used for a more efficient search when dealing with a large number of hyperparameters\n",
        "\n",
        "5. Evaluate Model Performance\n",
        "\n",
        "Assess the model's performance using metrics like accuracy, precision, recall, and F1-score. In healthcare applications, precision and recall are particularly important to minimize false positives and false negatives, respectively. Additionally, consider using a confusion matrix and ROC-AUC score to evaluate the model's ability to discriminate between classes\n",
        "\n",
        "Business Value\n",
        "\n",
        "Implementing this predictive model can significantly enhance patient care by enabling early detection of diseases, leading to timely interventions. It can also optimize resource allocation, reduce healthcare costs, and improve patient outcomes by identifying high-risk individuals who may benefit from preventive measures or closer monitoring.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "jkO8_Dpqrt_p",
        "outputId": "7bad3d02-475e-46d5-8304-4582d86cc6a5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n## Question 10: Imagine you’re working as a data scientist for a healthcare company that \\nwants to predict whether a patient has a certain disease. You have a large dataset with \\nmixed data types and some missing values. \\nExplain the step-by-step process you would follow to: \\n● Handle the missing values \\n● Encode the categorical features \\n● Train a Decision Tree model \\n● Tune its hyperparameters \\n● Evaluate its performance \\nAnd describe what business value this model could provide in the real-world \\nsetting. \\n\\n\\n\\nTo develop a predictive model for disease diagnosis using a healthcare dataset with mixed data types and missing values, follow these steps:\\n\\n1. Handle Missing Values\\n\\nUtilize the MissForest algorithm, a nonparametric imputation method, to handle missing data in mixed-type datasets. MissForest iteratively imputes missing values using a random forest, effectively capturing complex relationships between variables. This approach has been shown to outperform other imputation methods, especially when dealing with nonlinear interactions \\n\\n2. Encode Categorical Features\\n\\nApply one-hot encoding to transform categorical variables into binary columns. This method ensures that the model interprets categorical data appropriately, preventing any ordinal relationships from being inferred where none exist. For example, convert a 'Gender' column with values 'Male' and 'Female' into two separate columns: 'Gender_Male' and 'Gender_Female'.\\n\\n3. Train a Decision Tree Model\\n\\nUse a DecisionTreeClassifier from scikit-learn to train the model. Decision trees are suitable for healthcare data due to their interpretability and ability to handle both numerical and categorical features. Ensure to set parameters like max_depth and min_samples_split to prevent overfitting and enhance generalization.\\n\\n4. Tune Hyperparameters\\n\\nImplement GridSearchCV to perform an exhaustive search over specified parameter values, such as max_depth, min_samples_split, and min_samples_leaf. This method evaluates all combinations of parameters using cross-validation, ensuring the selection of the optimal model configuration. Alternatively, RandomizedSearchCV can be used for a more efficient search when dealing with a large number of hyperparameters \\n\\n5. Evaluate Model Performance\\n\\nAssess the model's performance using metrics like accuracy, precision, recall, and F1-score. In healthcare applications, precision and recall are particularly important to minimize false positives and false negatives, respectively. Additionally, consider using a confusion matrix and ROC-AUC score to evaluate the model's ability to discriminate between classes \\n\\nBusiness Value\\n\\nImplementing this predictive model can significantly enhance patient care by enabling early detection of diseases, leading to timely interventions. It can also optimize resource allocation, reduce healthcare costs, and improve patient outcomes by identifying high-risk individuals who may benefit from preventive measures or closer monitoring.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "it6GINvuugJN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}