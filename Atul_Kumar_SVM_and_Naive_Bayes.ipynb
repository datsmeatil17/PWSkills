{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "o0s3G634Dm2D",
        "outputId": "bc0485ec-54aa-4b70-dc8c-806bcdc27549"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n## Question 1:  What is a Support Vector Machine (SVM), and how does it work? \\nA Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates data points of different classes in a feature space. The goal is to maximize the margin between the hyperplane and the nearest data points from each class, known as support vectors.\\n\\nIn a simple two-dimensional case, the hyperplane is a line that divides the data into two classes. SVM selects the hyperplane that has the largest distance (margin) to the nearest training data point of any class. A larger margin is associated with better generalization and lower classification error on unseen data.\\n\\nSVM can also handle non-linear classification using a technique called the kernel trick. Kernels transform the input space into a higher-dimensional space where a linear separator can be found. Common kernels include polynomial, radial basis function (RBF), and sigmoid.\\n\\nSVMs are effective in high-dimensional spaces and are memory-efficient because they use a subset of training points in the decision function. However, they can be less effective on very large datasets and are sensitive to the choice of kernel and parameters like C (regularization) and gamma (in RBF kernel).\\n\\n\\n## Question 2: Explain the difference between Hard Margin and Soft Margin SVM. \\n\\nThe difference between Hard Margin and Soft Margin SVM lies in how strictly the model separates the data.\\n\\nHard Margin SVM\\n\\nHard Margin SVM is used when the data is linearly separable without any errors. It finds a hyperplane that perfectly separates the two classes with the maximum possible margin, ensuring no data points fall within the margin or are misclassified. While this approach leads to a clean separation, it is very sensitive to outliers and noise. Even a single misclassified point can make it impossible to find a separating hyperplane, making hard margin impractical for real-world, noisy datasets.\\n\\nSoft Margin SVM\\n\\nSoft Margin SVM allows some misclassifications or margin violations to improve the model’s generalization. It introduces a regularization parameter (C) that controls the trade-off between maximizing the margin and minimizing classification error. A small value of C allows more violations (leading to a wider margin), while a large C penalizes misclassifications more heavily (leading to a tighter fit).\\n\\nSoft margin SVM is more flexible and robust, especially with overlapping classes or noisy data, making it the preferred choice in most practical applications. It balances the need for accuracy with the ability to generalize well on unseen data.\\n\\n\\n## Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and \\nexplain its use case. \\n\\nThe Kernel Trick in Support Vector Machines (SVM) allows the algorithm to classify data that is not linearly separable by transforming it into a higher-dimensional space. Instead of manually transforming the data, the kernel trick uses a kernel function to compute the relationships between data points as if they were in that higher-dimensional space—without actually performing the transformation. This makes the process computationally efficient.\\n\\nExample: Radial Basis Function (RBF) Kernel\\n\\nThe RBF kernel, also known as the Gaussian kernel, is one of the most widely used kernels in SVM. It’s effective when the boundary between classes is curved or complex. The RBF kernel measures similarity based on the distance between data points, giving higher values to closer points.\\n\\nUse Case:\\n\\nThe RBF kernel is commonly used in applications like image recognition, spam detection, or medical diagnosis, where data cannot be separated with a straight line. For instance, in classifying handwritten digits, the data points (pixel intensities) form patterns that are not linearly separable. The RBF kernel allows SVM to capture those intricate patterns and make accurate predictions.\\n\\nIn short, the kernel trick lets SVM handle non-linear problems with the speed and efficiency of linear computation.\\n\\n## Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”? \\n\\nA Naïve Bayes Classifier is a simple yet powerful probabilistic machine learning algorithm used for classification tasks. It is based on Bayes’ Theorem, which describes the probability of a class given some features. Naïve Bayes is particularly popular for text classification, such as spam detection, sentiment analysis, and document categorization.\\n\\nThe classifier works by calculating the probability of each class based on the input features and then selecting the class with the highest probability. Despite its simplicity, it often performs surprisingly well, especially with large datasets.\\n\\nIt is called “naïve” because it makes a strong assumption: that all features are independent of each other given the class label. In reality, this assumption is rarely true—for example, in text data, words often appear together in patterns. However, the model still works well in practice even when this assumption is violated.\\n\\nThe Naïve Bayes classifier is fast, requires little training data, and handles high-dimensional inputs well, making it a good choice for initial models or real-time systems. However, it may not perform as well when feature dependencies play a crucial role in determining the outcome, as its assumption of independence can limit accuracy in such cases.\\n\\n## Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. \\nWhen would you use each one? \\n\\nNaïve Bayes has several variants, each designed for different types of data: Gaussian, Multinomial, and Bernoulli.\\n\\nGaussian Naïve Bayes\\n\\nThis variant assumes that the features follow a normal (Gaussian) distribution. It’s suitable for continuous data, such as age, height, or temperature. Gaussian Naïve Bayes is commonly used in problems like medical diagnosis or sensor data classification, where feature values are real numbers.\\n\\nMultinomial Naïve Bayes\\n\\nMultinomial Naïve Bayes is designed for discrete count data, such as the frequency of words in a document. It works well for text classification tasks like spam filtering, news categorization, and sentiment analysis. This model assumes that features (e.g., word counts) are drawn from a multinomial distribution and is particularly effective when dealing with document-term matrices.\\n\\nBernoulli Naïve Bayes\\n\\nBernoulli Naïve Bayes is used for binary/boolean features, where each feature represents whether a term is present or absent. It’s also used in text classification, but instead of using word frequency, it only considers whether a word appears at least once in a document. It's useful when the presence or absence of a feature matters more than its frequency.\\n\\nWhen to use which:\\n\\nGaussian: Continuous features\\n\\nMultinomial: Count data (e.g., word frequencies)\\n\\nBernoulli: Binary features (e.g., word presence\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "'''\n",
        "\n",
        "## Question 1:  What is a Support Vector Machine (SVM), and how does it work?\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that best separates data points of different classes in a feature space. The goal is to maximize the margin between the hyperplane and the nearest data points from each class, known as support vectors.\n",
        "\n",
        "In a simple two-dimensional case, the hyperplane is a line that divides the data into two classes. SVM selects the hyperplane that has the largest distance (margin) to the nearest training data point of any class. A larger margin is associated with better generalization and lower classification error on unseen data.\n",
        "\n",
        "SVM can also handle non-linear classification using a technique called the kernel trick. Kernels transform the input space into a higher-dimensional space where a linear separator can be found. Common kernels include polynomial, radial basis function (RBF), and sigmoid.\n",
        "\n",
        "SVMs are effective in high-dimensional spaces and are memory-efficient because they use a subset of training points in the decision function. However, they can be less effective on very large datasets and are sensitive to the choice of kernel and parameters like C (regularization) and gamma (in RBF kernel).\n",
        "\n",
        "\n",
        "## Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "The difference between Hard Margin and Soft Margin SVM lies in how strictly the model separates the data.\n",
        "\n",
        "Hard Margin SVM\n",
        "\n",
        "Hard Margin SVM is used when the data is linearly separable without any errors. It finds a hyperplane that perfectly separates the two classes with the maximum possible margin, ensuring no data points fall within the margin or are misclassified. While this approach leads to a clean separation, it is very sensitive to outliers and noise. Even a single misclassified point can make it impossible to find a separating hyperplane, making hard margin impractical for real-world, noisy datasets.\n",
        "\n",
        "Soft Margin SVM\n",
        "\n",
        "Soft Margin SVM allows some misclassifications or margin violations to improve the model’s generalization. It introduces a regularization parameter (C) that controls the trade-off between maximizing the margin and minimizing classification error. A small value of C allows more violations (leading to a wider margin), while a large C penalizes misclassifications more heavily (leading to a tighter fit).\n",
        "\n",
        "Soft margin SVM is more flexible and robust, especially with overlapping classes or noisy data, making it the preferred choice in most practical applications. It balances the need for accuracy with the ability to generalize well on unseen data.\n",
        "\n",
        "\n",
        "## Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "The Kernel Trick in Support Vector Machines (SVM) allows the algorithm to classify data that is not linearly separable by transforming it into a higher-dimensional space. Instead of manually transforming the data, the kernel trick uses a kernel function to compute the relationships between data points as if they were in that higher-dimensional space—without actually performing the transformation. This makes the process computationally efficient.\n",
        "\n",
        "Example: Radial Basis Function (RBF) Kernel\n",
        "\n",
        "The RBF kernel, also known as the Gaussian kernel, is one of the most widely used kernels in SVM. It’s effective when the boundary between classes is curved or complex. The RBF kernel measures similarity based on the distance between data points, giving higher values to closer points.\n",
        "\n",
        "Use Case:\n",
        "\n",
        "The RBF kernel is commonly used in applications like image recognition, spam detection, or medical diagnosis, where data cannot be separated with a straight line. For instance, in classifying handwritten digits, the data points (pixel intensities) form patterns that are not linearly separable. The RBF kernel allows SVM to capture those intricate patterns and make accurate predictions.\n",
        "\n",
        "In short, the kernel trick lets SVM handle non-linear problems with the speed and efficiency of linear computation.\n",
        "\n",
        "## Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "A Naïve Bayes Classifier is a simple yet powerful probabilistic machine learning algorithm used for classification tasks. It is based on Bayes’ Theorem, which describes the probability of a class given some features. Naïve Bayes is particularly popular for text classification, such as spam detection, sentiment analysis, and document categorization.\n",
        "\n",
        "The classifier works by calculating the probability of each class based on the input features and then selecting the class with the highest probability. Despite its simplicity, it often performs surprisingly well, especially with large datasets.\n",
        "\n",
        "It is called “naïve” because it makes a strong assumption: that all features are independent of each other given the class label. In reality, this assumption is rarely true—for example, in text data, words often appear together in patterns. However, the model still works well in practice even when this assumption is violated.\n",
        "\n",
        "The Naïve Bayes classifier is fast, requires little training data, and handles high-dimensional inputs well, making it a good choice for initial models or real-time systems. However, it may not perform as well when feature dependencies play a crucial role in determining the outcome, as its assumption of independence can limit accuracy in such cases.\n",
        "\n",
        "## Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "Naïve Bayes has several variants, each designed for different types of data: Gaussian, Multinomial, and Bernoulli.\n",
        "\n",
        "Gaussian Naïve Bayes\n",
        "\n",
        "This variant assumes that the features follow a normal (Gaussian) distribution. It’s suitable for continuous data, such as age, height, or temperature. Gaussian Naïve Bayes is commonly used in problems like medical diagnosis or sensor data classification, where feature values are real numbers.\n",
        "\n",
        "Multinomial Naïve Bayes\n",
        "\n",
        "Multinomial Naïve Bayes is designed for discrete count data, such as the frequency of words in a document. It works well for text classification tasks like spam filtering, news categorization, and sentiment analysis. This model assumes that features (e.g., word counts) are drawn from a multinomial distribution and is particularly effective when dealing with document-term matrices.\n",
        "\n",
        "Bernoulli Naïve Bayes\n",
        "\n",
        "Bernoulli Naïve Bayes is used for binary/boolean features, where each feature represents whether a term is present or absent. It’s also used in text classification, but instead of using word frequency, it only considers whether a word appears at least once in a document. It's useful when the presence or absence of a feature matters more than its frequency.\n",
        "\n",
        "When to use which:\n",
        "\n",
        "Gaussian: Continuous features\n",
        "\n",
        "Multinomial: Count data (e.g., word frequencies)\n",
        "\n",
        "Bernoulli: Binary features (e.g., word presence\n",
        "\n",
        "\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6:   Write a Python program to:\n",
        "#● Load the Iris dataset\n",
        "#● Train an SVM Classifier with a linear kernel\n",
        "#● Print the model's accuracy and support vectors.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Number of Support Vectors for each class:\", svm_model.n_support_)\n",
        "print(\"Support Vectors:\\n\", svm_model.support_vectors_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2Ox4gdyETyw",
        "outputId": "dfa64e33-ee03-4f67-954b-1b5124a35e7e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Number of Support Vectors for each class: [ 3 11 11]\n",
            "Support Vectors:\n",
            " [[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7:  Write a Python program to:\n",
        "#● Load the Breast Cancer dataset\n",
        "#● Train a Gaussian Naïve Bayes model\n",
        "#● Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXxrD893G76_",
        "outputId": "6393ee56-7d02-4410-b97a-e3a368190a47"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "#C and gamma.\n",
        "#● Print the best hyperparameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "# Create the SVM model and GridSearchCV\n",
        "svm = SVC()\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Test Set Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI1eicksHMDw",
        "outputId": "2df8c26e-aeaf-4bef-cd1d-cb1755af9503"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Test Set Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "#sklearn.datasets.fetch_20newsgroups).\n",
        "#● Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load a subset of the 20 Newsgroups dataset (binary classification)\n",
        "categories = ['sci.space', 'rec.sport.baseball']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(newsgroups.data)\n",
        "y = newsgroups.target  # 0 or 1\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Multinomial Naïve Bayes classifier\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for ROC-AUC\n",
        "y_proba = nb_model.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
        "\n",
        "# Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "# Print the result\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJsRdbVEHW1j",
        "outputId": "8b1ec830-1516-43e8-a76b-d73eeb0e00bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9964327574784692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "1. Data Preprocessing\n",
        "\n",
        "Text Cleaning & Vectorization:\n",
        "Emails often contain noisy text — punctuation, stopwords, and diverse vocabulary. I’d start by cleaning the text (removing special characters, lowercasing, removing stopwords if needed) and then convert the text into numerical features using TF-IDF vectorization. TF-IDF helps emphasize important words and reduce the impact of very common words.\n",
        "\n",
        "Handling Missing Data:\n",
        "If some emails have missing parts (e.g., empty body or subject), I’d either:\n",
        "\n",
        "Fill missing values with placeholders (like \"missing\") to retain info, or\n",
        "\n",
        "Remove or flag incomplete emails depending on how critical the missing data is.\n",
        "\n",
        "Feature Engineering:\n",
        "I might also include additional metadata features like email length, sender reputation, or presence of links to improve the model.\n",
        "\n",
        "2. Model Choice: SVM vs. Naïve Bayes\n",
        "\n",
        "Naïve Bayes is often the go-to for spam detection because it’s fast, handles high-dimensional text well, and performs surprisingly well despite its simplistic assumptions.\n",
        "\n",
        "SVM with a linear kernel can also perform strongly, especially if the data is well-preprocessed and large enough, as it finds an optimal boundary with good generalization.\n",
        "\n",
        "Given the class imbalance and diverse vocabulary, I’d start with Naïve Bayes because it’s robust with text data and easier to tune. If needed, I’d try SVM next for potentially improved accuracy.\n",
        "\n",
        "3. Addressing Class Imbalance\n",
        "\n",
        "Spam datasets usually have fewer spam emails compared to legitimate ones, which can bias the model.\n",
        "\n",
        "To tackle this:\n",
        "\n",
        "Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) or random oversampling to balance the classes in training data.\n",
        "\n",
        "Alternatively, apply class weighting (e.g., in SVM) to penalize misclassifying the minority class more.\n",
        "\n",
        "Also, choose metrics sensitive to imbalance (more on that below).\n",
        "\n",
        "4. Performance Evaluation\n",
        "\n",
        "Accuracy alone can be misleading due to imbalance, so I’d rely on:\n",
        "\n",
        "Precision: How many emails flagged as spam are actually spam? (Avoid false positives)\n",
        "\n",
        "Recall: How many actual spam emails are correctly identified? (Avoid missing spam)\n",
        "\n",
        "F1-score: Balance between precision and recall.\n",
        "\n",
        "ROC-AUC or PR-AUC: Measures overall model discrimination power, especially useful with imbalance.\n",
        "\n",
        "I’d also monitor false positives carefully, since marking legitimate emails as spam can hurt user trust and business.\n",
        "\n",
        "Business Impact\n",
        "\n",
        "A reliable spam filter:\n",
        "\n",
        "Improves user experience by keeping inboxes clean and focused.\n",
        "\n",
        "Saves time and resources by reducing manual email sorting and spam-related risks.\n",
        "\n",
        "Protects against phishing and malware, improving company security.\n",
        "\n",
        "Builds user trust in the company’s communication system.\n",
        "\n",
        "However, minimizing false positives is critical — incorrectly classifying legitimate emails as spam could result in lost opportunities, frustrated users, and damage to the company’s reputation.\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "iy1-f19FHnxg",
        "outputId": "7f526c38-a803-4547-8196-36ba8336ea8b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Question 10: Imagine you’re working as a data scientist for a company that handles \\nemail communications. \\nYour task is to automatically classify emails as Spam or Not Spam. The emails may \\ncontain: \\n● Text with diverse vocabulary \\n● Potential class imbalance (far more legitimate emails than spam) \\n● Some incomplete or missing data \\nExplain the approach you would take to: \\n● Preprocess the data (e.g. text vectorization, handling missing data) \\n● Choose and justify an appropriate model (SVM vs. Naïve Bayes) \\n● Address class imbalance \\n● Evaluate the performance of your solution with suitable metrics \\nAnd explain the business impact of your solution. \\n\\n1. Data Preprocessing\\n\\nText Cleaning & Vectorization:\\nEmails often contain noisy text — punctuation, stopwords, and diverse vocabulary. I’d start by cleaning the text (removing special characters, lowercasing, removing stopwords if needed) and then convert the text into numerical features using TF-IDF vectorization. TF-IDF helps emphasize important words and reduce the impact of very common words.\\n\\nHandling Missing Data:\\nIf some emails have missing parts (e.g., empty body or subject), I’d either:\\n\\nFill missing values with placeholders (like \"missing\") to retain info, or\\n\\nRemove or flag incomplete emails depending on how critical the missing data is.\\n\\nFeature Engineering:\\nI might also include additional metadata features like email length, sender reputation, or presence of links to improve the model.\\n\\n2. Model Choice: SVM vs. Naïve Bayes\\n\\nNaïve Bayes is often the go-to for spam detection because it’s fast, handles high-dimensional text well, and performs surprisingly well despite its simplistic assumptions.\\n\\nSVM with a linear kernel can also perform strongly, especially if the data is well-preprocessed and large enough, as it finds an optimal boundary with good generalization.\\n\\nGiven the class imbalance and diverse vocabulary, I’d start with Naïve Bayes because it’s robust with text data and easier to tune. If needed, I’d try SVM next for potentially improved accuracy.\\n\\n3. Addressing Class Imbalance\\n\\nSpam datasets usually have fewer spam emails compared to legitimate ones, which can bias the model.\\n\\nTo tackle this:\\n\\nUse techniques like SMOTE (Synthetic Minority Over-sampling Technique) or random oversampling to balance the classes in training data.\\n\\nAlternatively, apply class weighting (e.g., in SVM) to penalize misclassifying the minority class more.\\n\\nAlso, choose metrics sensitive to imbalance (more on that below).\\n\\n4. Performance Evaluation\\n\\nAccuracy alone can be misleading due to imbalance, so I’d rely on:\\n\\nPrecision: How many emails flagged as spam are actually spam? (Avoid false positives)\\n\\nRecall: How many actual spam emails are correctly identified? (Avoid missing spam)\\n\\nF1-score: Balance between precision and recall.\\n\\nROC-AUC or PR-AUC: Measures overall model discrimination power, especially useful with imbalance.\\n\\nI’d also monitor false positives carefully, since marking legitimate emails as spam can hurt user trust and business.\\n\\nBusiness Impact\\n\\nA reliable spam filter:\\n\\nImproves user experience by keeping inboxes clean and focused.\\n\\nSaves time and resources by reducing manual email sorting and spam-related risks.\\n\\nProtects against phishing and malware, improving company security.\\n\\nBuilds user trust in the company’s communication system.\\n\\nHowever, minimizing false positives is critical — incorrectly classifying legitimate emails as spam could result in lost opportunities, frustrated users, and damage to the company’s reputation.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UqJY91DnHy5e"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}